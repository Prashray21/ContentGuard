# ğŸ›¡ï¸ ContentGuard

**ContentGuard** is an AI-powered NSFW image detection system that instantly analyzes and flags explicit content to help you protect your users and your brand.  
Built using **Flask** and a **state-of-the-art deep learning model**, it provides fast, reliable, and automated content moderation â€” ensuring a safer online environment for everyone.

---

## ğŸš€ Features

- âš¡ **Real-time Detection:** Instantly analyze uploaded images for NSFW or explicit content.  
- ğŸ§  **AI-Powered Model:** Uses a deep learning model trained on diverse datasets for high accuracy.  
- ğŸŒ **Flask Backend:** Lightweight and efficient Python backend for serving predictions.  
- ğŸ§© **Easy Integration:** Simple API endpoints for seamless integration into any web or mobile platform.  
- ğŸ”’ **Privacy-Focused:** No data stored â€” all analysis happens securely on the server.

---

## ğŸ—ï¸ Tech Stack

- **Backend:** Flask (Python)  
- **Model:** Deep Learning (NSFW image detection model)  
- **Frontend:** HTML, CSS, JavaScript  
- **Environment:** Python 3.10+  

---

## ğŸ“¦ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Prashray21/ContentGuard
   cd ContentGuard
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Flask app:**
   ```bash
   python app.py
   ```

4. **Open your browser** and visit  
   ğŸ‘‰ [http://127.0.0.1:5000](http://127.0.0.1:5000)

---

## âš™ï¸ API Endpoints

| Method | Endpoint | Description |
|--------|-----------|-------------|
| `POST` | `/analyze` | Upload an image and get NSFW prediction results |
| `GET`  | `/` | Returns the home page / upload interface |

Example Request:
```bash
curl -X POST -F "file=@example.jpg" http://127.0.0.1:5000/predict
```

Response:
```json
{
  "nsfw_score": 0.92,
  "safe": false
}
```

---

## ğŸ§  Model Details

The model is a fine-tuned CNN-based NSFW classifier trained on multiple open-source datasets for detecting:
- Pornographic content  
- Nudity  
- Suggestive material  
- Safe content  

The prediction score (`nsfw_score`) ranges between **0** (safe) and **1** (explicit).

---

## ğŸ§‘â€ğŸ’» Project Structure

```
ContentGuard/
â”‚
â”œâ”€â”€ app.py                  # Flask app entry point
â”œâ”€â”€ style.css               # CSS file
â”œâ”€â”€ script.js               # JS file
â”œâ”€â”€ index.html              # HTML templates
â”œâ”€â”€ nsfw_image_detection/   # Trained NSFW detection model
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ“ˆ Future Improvements

- ğŸš€ Add video and GIF NSFW detection  
- ğŸ¤– Integrate better multi-class labeling (e.g., hentai, suggestive, neutral)  
- â˜ï¸ Deploy on cloud (Render / AWS / Hugging Face Spaces)  
- ğŸ§© Add authentication for API endpoints  

---

## ğŸ› ï¸ Contributing

Pull requests are welcome!  
For major changes, please open an issue first to discuss what youâ€™d like to change.

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ’¬ Acknowledgments

- [FalconsAI NSFW Detection Model](https://github.com/Falconsai/nsfw_image_detection)  
- [Flask](https://flask.palletsprojects.com/)  
- [TensorFlow](https://www.tensorflow.org/) / [PyTorch](https://pytorch.org/)

---

### âœ¨ Made with â¤ï¸ by Our Team
